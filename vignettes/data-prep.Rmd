---
title: "2. Understanding the Trade-offs: Data Preparation for Spatial Accessibility Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{2. Understanding the Trade-offs: Data Preparation for Spatial Accessibility Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 4.5,
  dpi = 150
)

# Load required packages
library(spax)
library(terra)
library(pryr)  # for memory tracking
library(tidyverse)
library(sf)
library(osrm)
```

# The Raster Approach: Understanding the Trade-offs

When working with spatial accessibility analysis, one of the first decisions is how to represent spatial data. The `spax` package takes a raster-based approach (at least for now), representing space as a grid of regular cells rather than points and polygons. This fundamental choice shapes everything from memory usage to computational methods.

## Memory vs. Computational Simplicity

Let's be upfront about the costs and benefits. Raster representations typically require more memory than their vector counterparts:

```{r memory-comparison}
# Compare memory usage between vector and raster isochrones
vec_size <- object_size(hos_iscvec)/1024^2
rast_size <- object_size(hos_iscr)/1024^2

cat("Memory usage comparison:\n",
    sprintf("Vector isochrones: %.1f MB\n", vec_size),
    sprintf("Raster isochrones: %.1f MB\n", rast_size),
    sprintf("Memory increase factor: %.1fx\n", rast_size/vec_size))
```

As we can see, raster isochrones are 32.7 times larger than vector isochrones\*. So, why accept this memory overhead? The benefits come down to algorithmic simplicity and computational consistency:

> **\*Note:** The actual memory increase factor may vary depending on the resolution and complexity of the raster data.

#### 1. Computational Efficiency

Raster operations translate directly into matrix operations, which modern computers are highly optimized to handle. Instead of complex geometric calculations, we're essentially doing math on a grid:

```{r}
# Simple example: 
# 1. Calculate areas within 30 minutes of a facility
# 2. Calculate Gaussian distance decay with an SD of 30 minutes
# 3. Calculate Exponential distance decay with a decay rate of 0.05

distance_raster <- rast(hos_iscr)[[1]]  # Travel time to first facility

# Set grid
par(mfrow = c(2, 2))

# Distance raster
plot(distance_raster, main = "Travel Time to Hospital C1 (minutes)")
plot(vect(hc12_hos[1,]), add = TRUE, col = "red", pch = 16)
plot(vect(bound0), add = TRUE)


# Area within 30 minutes
within_30min <- distance_raster <= 30    # Simple logical operation
plot(within_30min, main = "Area within 30 minutes")
plot(vect(hc12_hos[1,]), add = TRUE, col = "red", pch = 16)
plot(vect(bound0), add = TRUE)

# Gaussian decay
gaussian_decay <- exp(-0.5 * (distance_raster / 30)^2)  #
plot(gaussian_decay, main = "Gaussian Distance Decay")
plot(vect(hc12_hos[1,]), add = TRUE, col = "red", pch = 16)
plot(vect(bound0), add = TRUE)

# Exponential decay
exponential_decay <- exp(-0.05 * distance_raster)
plot(exponential_decay, main = "Exponential Distance Decay")
plot(vect(hc12_hos[1,]), add = TRUE, col = "red", pch = 16)
plot(vect(bound0), add = TRUE)
```

#### 2. Data Structure Consistency

Almost everything - population, travel times, accessibility scores - lives in the same format (maybe except supply-demand ratio calulation that performs as a vector). This eliminates the need to convert between different spatial representations:

```{r}
# Show how different components share the same structure
par(mfrow = c(1, 3))

# Population density
plot(rast(u5pd), main = "Population")

# Travel time
plot(distance_raster, main = "Travel Time")

# Result will have same structure
plot(within_30min, main = "Analysis Result")
```

#### 3. Built-in Spatial Relationships

The grid structure automatically encodes spatial relationships. We don't need separate distance matrices or spatial indices - the cell positions themselves tell us what's near what.

## When to Consider Alternatives

The raster approach isn't always optimal. Consider vector-based methods when:

-   Your study area is very sparse with few service locations
-   You need exact precision at specific points
-   Memory is severely constrained and you're working with a large area
-   The granularity of your analysis doesn't require a continuous surface

# Data Preparation for Accessibility Analysis

> *“Give me six hours to chop down a tree, and I will spend the first four sharpening the axe.”*
>
> — Often attributed to Abraham Lincoln

## Dealing with Demand

When preparing demand data for spatial accessibility analysis, we have several approaches available, each with its own assumptions and trade-offs. Let's explore these methods using a simple example of disease cases by district.

```{r simulate cases}
set.seed(42) # For reproducibility

# Example district-level case data
case_data <- tibble(
  ADM1_PCODE = bound1$ADM1_PCODE,
  cases = round(rnorm(nrow(bound1), 1000, 200))  # Simulated disease cases
)

# Join with spatial data
case_spatial <- bound1 %>%
  left_join(case_data, by = "ADM1_PCODE")
```

In a vector-based approach, we typically represent each area with a polygon that has an attribute for the number of cases. This often results in choropleth maps, like this:

```{r}
# Plot the case
ggplot() +
  geom_sf(data = bound1) +
  geom_sf(data = case_spatial, aes(fill = cases)) +
  labs(title = "Cases by Province") +
  scale_fill_viridis_b()+
  theme_minimal()
```

But that could sometimes be misleading as when performing spatial analysis; we often allocate demand to only at the centroid of each area, like this:

```{r}
# Calculate centroids and rasterize
centroids <- st_centroid(case_spatial)

# Plot the population data
ggplot() +
  geom_sf(data = bound1) +
  geom_sf(data = centroids, aes(size = cases), col = "red") +
  labs(title = "Population Data by Region") +
  theme_minimal()
```

and then we proceed to estimate the pair distance between each centroid-facility pair to calculate the decay weight and accessibility score.

Now, let's see how we can deal with the demand in a raster-centric method.

### 1. Centroid-Based Approach

The simplest approach treats all demand as concentrated at area centroids. This mirrors traditional vector-based accessibility analyses:

```{r admin_pop_data}
# Create template raster - Coarser resolution for example
template <- aggregate(rast(u5pd), fact = 10)  

# Rasterize centroids
centroid_demand <- terra::rasterize(
  vect(centroids), 
  template,
  field = "cases", 
  fun = "sum"
)
```

```{r}
plot(centroid_demand, main = "Centroid-Based Demand")
plot(vect(bound1), add = TRUE)
```

As you can see here, the centroid vector data has been converted into a raster format (despite its weird-looking result). However, this raster can now be used in the accessibility analysis, just like the original population density raster.

+-------------------------+--------------------------------------+
| Pros                    | Cons                                 |
+=========================+======================================+
| -   Simple to implement | -   Unrealistic spatial distribution |
+-------------------------+--------------------------------------+

### 2. Area-Weighted Approach [developing]

Now, we can really take a choropleth map into analysis. This method assumes cases are uniformly distributed across each district:

```{r}
# test
case_spatial_spat <- vect(case_spatial)
case_spatial_spat |> expanse()

terra::rasterize(case_spatial_spat, template, field = 1)  |> plot()
```

```{r}
# 1. Calculate case density
case_spatial <- case_spatial |>
  mutate(
    area_m2 = st_area(case_spatial),  # estimate area
    case_density = as.numeric(cases / area_m2)  # calc density
  )

# 2. Rasterize case density
case_density_raster <- terra::rasterize(
  vect(case_spatial),
  template,
  field = "case_density"
)

# 3. Calculate pixel area in square kilometers
pixel_area <- terra::res(template)[1] * terra::res(template)[2]

# 4. Convert density to absolute cases per pixel
case_raster <- case_density_raster * pixel_area
```

And you would have some thing like this:

```{r}
# Plot 
plot(case_raster, main = "Area-Weighted Demand")
plot(vect(bound1), add = TRUE)
```

```{r}
# Extract raster values for each polygon
extracted_cases <- terra::extract(case_raster, vect(case_spatial), fun = sum, na.rm = TRUE)
case_spatial <- case_spatial %>%
  mutate(
    raster_cases = extracted_cases[, 2]  # Extracted values
  )

# Compare the result
case_spatial |> st_drop_geometry() |>
  select(ADM1_PCODE, cases, raster_cases)

# Due to the demand approximation, the raster-based method may not exactly match the original case counts. The results, though, will geting closer as the resolution increas.es
```

### 3. Informed Distribution [developing]

This approach uses auxiliary data (like population density) to distribute cases more realistically:

### 4. Probabilistic Sampling [developing]

For uncertainty analysis, spax provides the sample_pmf() function to generate multiple realizations of demand distributions:

```{r}
# This advanced approach is covered in detail in the "Probabilistic Accessibility Analysis" vignette (developing)
?sample_pmf  # See documentation for details
```

The choice of demand preparation method can significantly impact your accessibility analysis results. Consider your specific context:

-   Data availability (Do you have population density data?)
-   Scale of analysis (Are districts large enough that distribution matters?)
-   Purpose (Is point-based analysis sufficient for your needs?)
-   Computational resources (Can you handle higher resolution distributions?) \### Processing Supply Data & Distance Raster

## Processing Supply and Distance Data

### Supply Data: The Known Part of the Equation

Supply locations are typically the most straightforward component of accessibility analysis - we usually know exactly where facilities are and their capacities. However, there are still important considerations in preparing this data.

```{r}
# Example of typical supply data structure
head(hc12_hos) |>
  select(id, hoslvl, s_doc, s_nurse) 
```

Supply data typically starts as point locations with attributes (as it's usually a known part of our equation). Let's say we have a dataset of hospital capacity in the region, and we geo-located each of them, you might get something like this:

```{r}
# Dissect the original data 
head(hc12_hos)
class(hc12_hos)
```

For use in `spax`, supply data should be separated into:

1.  Spatial locations (for distance calculations)
2.  Capacity attributes (for accessibility computations)

```{r}
# Separate spatial and attribute components
supply_attributes <- hc12_hos |>
  st_drop_geometry() |>
  select(id, s_doc, s_nurse)  # Keep only relevant columns

# Visual check of supply distribution
ggplot() +
  geom_sf(data = bound1, fill = "grey80") +
  geom_sf(data = hc12_hos, aes(size = s_doc, color = hoslvl)) +
  scale_size_continuous(name = "Number of Doctors") +
  scale_color_viridis_d(name = "Hospital Level", end = 0.5) +
  theme_minimal() +
  labs(title = "Healthcare Supply Distribution",
       subtitle = "Size indicates number of doctors, color shows facility level")
```

### Creating Distance/Travel Time Surfaces

On the other hand, the most complex part of data preparation, in my opinion, is often creating travel time or distance surfaces. You have several options:

#### 1. Euclidean Distance

The simplest approach - straight-line distances:

```{r}
# Example for one facility using terra
facility_point <- hc12_hos[1,]
template_rast <- rast(u5pd)  # Use population raster as template

# Calculate Euclidean distance (in meters since our CRS is projected)
euclidean_dist <- terra::distance(
  template_rast, 
  vect(facility_point)
) |> crop(vect(bound0), mask = TRUE)
```

```{r}
# Plot the Euclidean distance
plot(euclidean_dist / 1000, # Convert to kilometers
     main = "Euclidean Distance (km)")
plot(vect(facility_point), add = TRUE, col = "red", pch = 16)
plot(vect(bound0), add = TRUE)
```

And then you can loop over all facilities to calculate the distance raster for each facility.

#### 2. Travel Time Isochrones

Another way to go is to create travel time isochrones, which represent areas reachable from a location within a given time threshold. This package did not provide a function to generate isochrones, but we can use the [`osrm`](https://github.com/riatelab/osrm){.uri} package to calculate travel times rings around each supply location.

```{r warning=FALSE}
# Example: Calculate isochrones for the first hospital
# Since the dataset I've prepared is in a planar projection, we need to transform it to WGS84 (EPSG:4326) for routing services

(example_hos1 <- hc12_hos[1, ] |> # Select the first hospital
  st_transform(4326) |> # Transform to WGS84 (geodesic projection)
  st_coordinates() |> # Extract coordinates
  as.numeric()) # Convert to numeric

# Calculate isochrones
example_isochrone <- osrmIsochrone(
  loc = example_hos1, # Example coordinates
  breaks = c(15, 30, 45, 60), # 15-minute intervals
  res = 30 
)

# change crs back
(example_isochrone <- st_transform(example_isochrone, st_crs(bound0)))
par(mfrow = c(1, 2))
plot(example_isochrone[, "isomax"], main = "Travel Time Isochrone for Hospital C1")
plot(example_isochrone[3, "isomax"], main = "Travel Time Isochrone ring for Hospital C1 (45 mins)")
```

As you can see, osrmIsochrone calculates the area reachable within each time threshold for the first hospital (C1) in 15-minute intervals up to 60 minutes as distinct ring-shaped vectors. If you take on a vector-based approach, you can use functions like `st_contain()` to determine the population within each isochrone ring. But in our raster-centric package, we'll rasterize these isochrones for further analysis.

```{r}
# Create a iso_mean column, and id
example_isochrone <- example_isochrone |>
  mutate(
    iso_mean = (isomin + isomax) / 2,
    location_id = hc12_hos[1, ]$id # actually dont need it here
)

# Convert isochrones to raster format
example_raster <- terra::rasterize(
  x = example_isochrone,
  y = template_rast, 
  field = "iso_mean", # Fill the raster with the isomax values
  background = NA,
  fun = "sum",
  by = "location_id") |>
  crop(vect(bound0), mask = TRUE)

```

```{r}
# Plot
plot(example_raster, main = "Travel Time Isochrone for Hospital C1")
plot(vect(bound0), add = TRUE)
plot(vect(hc12_hos[1, ]), add = TRUE, col = "red", pch = 16)
```

To analyze all hospitals, we can iterate through each facility and compute their isochrones. By default, the osrmIsochrone function uses the OSRM demo server to perform these calculations. However, relying on the demo server is not recommended for large datasets or repeated analyses due to usage limits and ethical considerations. Instead, it’s advisable to [set up your own OSRM server](https://github.com/Project-OSRM/osrm-backend). This vignette does not cover the server setup process.

```{r, eval=FALSE}
# Example of looping over all hospitals to calculate isochrones
# Note: This code is for illustration only and may not run due to API limitations
# Initialize a list to store isochrones
isochrones <- list(rep(NA, nrow(hc12_hos)))

# Extract coordinates
hos_coords <- hc12_hos |>
  st_transform(4326) |> # only if the data is in planar projection
  st_coordinates()

# Loop over each hospital - lapply
isochrones <- lapply(1:nrow(hc12_hos), function(i) {
  # Calculate isochrone
  osrmIsochrone(
    loc = hos_coords[i, ],
    breaks = c(15, 30, 45, 60),
    res = 30
  )
}) 
# Then proceed to rastarization ...
```

### Service Provider Options

**OSRM (Open Source Routing Machine)**

-   Free and open source
-   Requires setting up your own server for large analyses
-   Good for road-based travel times (you can actually use it for other traveling modes, but that requires more setup)

**HERE, Google Maps, or other APIs**

-   More accurate traffic data
-   Cost associated with usage
-   May have usage limits

**Custom cost surfaces** (I have no idea how to do this, lol)

-   Can incorporate terrain, land use
-   Requires more GIS expertise
-   Good for non-road-based accessibility

## Memory Considerations

Travel time surfaces often dominate memory usage. For *n* facilities and a raster of *m* cells, you need space for *n × m* values:

```{r}
# Calculate memory requirements
cells_per_raster <- ncell(rast(u5pd))
n_facilities <- nrow(hc12_hos)
bytes_per_value <- 8  # Double precision float

total_mb <- cells_per_raster * n_facilities * bytes_per_value / 1024^2

cat(sprintf("Memory required for ~520*520 m^2 resolution: %.1f MB\n", total_mb))

# Option: Reduce resolution for initial analysis
cells_per_raster_2 <- ncell(rast(aggregate(u5pd, fact = 2)))
total_mb_2 <- cells_per_raster_2 * n_facilities * bytes_per_value / 1024^2

cat(sprintf("Memory required for ~1040*1040 m^2 resolution: %.1f MB\n", total_mb_2))
```

**Tips for managing memory:**

-   Start with lower resolution for testing
-   Consider using chunks for large spatial datasets
-   Clean up unused objects during processing

## Validating Your Data: Essential Checks Before Analysis

Before running any accessibility analysis, it's crucial to validate your prepared data. Here's a systematic approach to validation (some of which already implemented in the package:) :

#### 1. Spatial Alignment

All raster inputs must align perfectly - same extent, resolution, and projection:

```{r}
# Example:
# You might want to create function to summarize raster properties
summarize_raster <- function(r, name) {
  tibble(
    Dataset = name,
    Resolution = paste(round(res(r),2) , collapse = " x "),
    Extent = paste(round(as.vector(ext(r)), 2), collapse = ", "),
    CRS = crs(r, proj = TRUE)
  )
}

# Compare properties of key rasters
raster_props <- bind_rows(
  summarize_raster(rast(u5pd), "Population"),
  summarize_raster(rast(hos_iscr), "Travel Time")
)

raster_props

# Visual check for alignment
plot(rast(u5pd), main = "Spatial Alignment Check")
plot(rast(hos_iscr)[[1]], add = TRUE, alpha = 0.5)
plot(vect(bound1), add = TRUE)
```

#### 2. Value Range Validation

Check for reasonable values in your input data:

```{r}
# Check population density range
summary(u5pd)
summary(hos_iscr[[1:10]])  # Check a few travel time rasters
```

#### 3. Missing Value Analysis

Understand where and why you have NA values; note that not all NAs are bad:

```{r}
# Function to analyze NA patterns within boundary
analyze_nas <- function(r, name, bound = bound0) {
  # Create mask from boundary
  mask <- terra::rasterize(bound, r)
  
  # Count cells only within boundary
  cells_in_boundary <- global(!is.na(mask), "sum")$sum
  nas_in_boundary <- global(is.na(r) & !is.na(mask), "sum")$sum
  
  cat(sprintf("%s:\n", name),
      sprintf("  NA cells within boundary: %d (%.1f%% of study area)\n", 
              nas_in_boundary, 
              100 * nas_in_boundary/cells_in_boundary))
  
  # Visualize NA pattern
  na_map <- (is.na(r) & !is.na(mask)) |> crop(bound, mask = T)
  
  plot(na_map, main = paste(name, "- NAs within Study Area"),
       col = c("grey80", "red"),
       legend = FALSE)
  plot(vect(bound1), add = TRUE)
  legend("bottomright", 
         legend = c("Data present", "NA"),
         fill = c("grey80", "red"),
         bty = "n")
}

# Check NA patterns in key datasets
par(mfrow = c(1, 2))
analyze_nas(rast(u5pd), "Pop")
analyze_nas(rast(hos_iscr)[[1]], "Time from 1st Fac")
```

### Final reminders:

1.  **Resolution Selection** 

    -   Higher resolution ≠ better analysis
    -   Consider computational resources
    -   Match resolution to spatial scale of research question

2.  **Coordinate Reference Systems**

    -   Be consistent with CRS across all datasets
    -   Document CRS choices and transformations

------------------------------------------------------------------------

# Next Steps

With your data prepared, you're ready to move on to actual accessibility analysis. The next vignette will cover how `spax` functions work together to compute accessibility scores.
