---
title: "4. Why Rasters? The Pros, Cons, and Best Practices"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{4. Why Rasters? The Pros, Cons, and Best Practices}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 4.5,
  dpi = 150
)
```

# Why Rasters? The Pros, Cons, and Best Practices

If you've worked through the previous vignettes, you've seen how spax relies heavily on raster operations for spatial accessibility analysis. But why did we choose this approach? And more importantly, what does it mean for your analysis? In this vignette, we'll explore the practical implications of using raster-based operations, looking at both the benefits and challenges, and providing concrete strategies for optimizing your workflows.

```{r setup, warning = FALSE, message = FALSE}
# Load required packages
library(spax)
library(terra)
library(pryr) # for memory tracking
library(tidyverse)
library(sf)
library(bench) # fotr benchmarking
```

## Understanding the Raster Approach

Think of a raster as a giant spreadsheet laid over your study area. Instead of tracking individual points or complex polygons, we store values in a regular grid of cells. This might seem simple, but it's this very simplicity that makes raster operations so powerful for accessibility analysis. Let's see this in action with our example data:

```{r}
# Load example population density
pop <- rast(u5pd)

# Quick look at what we're working with
(pop)
cat("Total cells:", ncell(pop), "\n")
```

When we calculate accessibility, nearly everything becomes a grid operation:

-   Population? A grid of density values\
-   Travel time? A grid for each facility\
-   Distance decay? A transformation of those grids\
-   Final accessibility? You guessed it - another grid

Spatial accessibility often boils down to “map algebra”:

$\text{Some function of (Population density)} \times \text{(Distance weights)}.$

Rasters make that algebra super direct: you take your population raster, multiply by a distance raster, and sum the result. Libraries like [*`terra`*](https://rspatial.github.io/terra/reference/terra-package.html "terra main page") are built for these cell-by-cell operations, and `spax` simply arrange them in a handy workflow.

## The Good: Why Raster Operations Shine

#### 1. Vectorized Operations = Efficient Computation

One of the biggest advantages of raster operations is that they're naturally vectorized. Instead of looping through points or polygons, we can perform operations on entire surfaces at once. Let's see this in practice:

```{r}
# Example: Calculating Gaussian decay for travel times
# This one line transforms EVERY cell in our raster
system.time({
  decay_weights <- calc_decay(
    distance = rast(hos_iscr)[[1]], 
    method = "gaussian",
    sigma = 30
  )
})

# Visualize the result
plot(decay_weights, main="Distance Decay Weights")
plot(vect(bound0), add=TRUE)
```

The operation above processes hundreds of thousands of cells in a single step, leveraging optimized matrix operations under the hood.

#### 2. Consistent Data Structures

Once you decide on a raster resolution (e.g., 500m cells), *everything*—population, distance, decayed weight, final accessibility—can be stored in the same grid. That means fewer mismatches, repeated coordinate transformations, or swapping between point sets and polygons. You’re just piling up new layers on the same “sheet.”

```{r}
# Look at alignment of our key datasets
check_alignment <- function(rast1, rast2, name1, name2) {
  cat("\nComparing", name1, "and", name2, ":\n")
  cat("Same resolution?", all(res(rast1) == res(rast2)), "\n")
  cat("Same extent?", all(ext(rast1) == ext(rast2)), "\n")
  cat("Same CRS?", crs(rast1) == crs(rast2), "\n")
}

# Check population and distance rasters
check_alignment(
  rast(u5pd), 
  rast(hos_iscr), 
  "Population", 
  "Distance"
)
```

#### 3. Built-in Spatial Relationships

The grid structure automatically encodes spatial relationships - no need for complex spatial indices or distance matrices. Want to know what's nearby? Just look at neighboring cells:

```{r}
# Look at one facility's service area
facility_id <- hc12_hos$id[1]  # First facility
facility_distance <- rast(hos_iscr)[[1]]

facility_distance <- terra::focal(
  facility_distance, 
  w = matrix(1, 21, 21), fun = mean, na.rm = T
)

# Bring back the custom decay
custom_decay <- function(distance, sigma = 45, threshold = 120) {
  gaussian <- exp(-distance^2 / (2 * sigma^2))
  gaussian[distance > threshold] <- NA  # Hard cutoff at threshold
  return(gaussian)
}

# Calculate decay weights
decay_weights <- calc_decay(
  facility_distance, 
  method = custom_decay
)

# The population raster automatically masks unpopulated areas
weighted_pop <- rast(u5pd) * decay_weights

# Visualize how spatial relationships are preserved
par(mfrow = c(2, 2))

plot(rast(u5pd), main = "Population")
plot(vect(bound0), add = TRUE)

plot(decay_weights, main = "Distance Decay")
plot(vect(bound0), add = TRUE)

plot(weighted_pop, main = "Population-Weighted Decay")
plot(vect(bound0), add = TRUE)
```

## The Trade-Offs: The Trade-offs: Resolution, Precision, and Computation

After exploring the benefits of raster operations, it's time to confront their fundamental limitations. The key trade-off in raster-based analysis is between computational efficiency and spatial precision - and it affects everything from population distribution to final accessibility scores.

### The Resolution Dilemma

Every raster analysis starts with a critical choice: what cell size should we use? This seemingly simple decision has cascading effects throughout our analysis. Let's explore this with a concrete example:

```{r}
# Test how resolution affects both precision and computation
pop <- rast(u5pd)
dist <- rast(hos_iscr)

test_facility_catchment <- function(factor) {
  # Time the entire operation
  start_time <- Sys.time()
  
  # Aggregate both population and distance
  pop_agg <- aggregate(pop, fact = factor, fun = sum, na.rm = TRUE)
  dist_agg <- aggregate(dist, fact = factor, fun = mean, na.rm = TRUE)
  
  # Calculate catchment areas
  weights <- calc_decay(dist_agg, method = "gaussian", sigma = 30)
  facility_demand <- gather_demand(pop_agg, weights)
  
  end_time <- Sys.time()
  
  # Return comprehensive results
  list(
    resolution = res(pop_agg)[1],
    ncells = ncell(pop_agg),
    runtime = as.numeric(difftime(end_time, start_time, units = "secs")),
    demands = facility_demand$potential_demand,
    pop = pop_agg,
    weights = weights
  )
}

# Test a range of resolutions
factors <- c(1, 2, 4, 8)  # Each step doubles cell size
results <- lapply(factors, test_facility_catchment)

# Create summary table
summary_table <- tibble(
  'Resolution (m)' = sapply(results, function(x) round(x$resolution)),
  'Grid Cells' = sapply(results, function(x) format(x$ncells, big.mark=",")),
  'Runtime (s)' = sapply(results, function(x) round(x$runtime, 2))
)

# Performance Impact of Resolution:
(summary_table)
```

But what does this resolution change actually mean for our analysis? Let's look at how it affects our catchment calculations:

```{r}
# Compare catchment populations for first few facilities
catchment_table <- data.frame(
  Resolution = paste0(round(sapply(results, `[[`, "resolution")), "m")
)

# Add results for first three facilities
for(i in 1:3) {
  catchment_table[paste("Facility", i, "Catchment")] <- sapply(results, 
    function(x) round(x$demands[i]))
}

print("\nCatchment Population Estimates:")
print(catchment_table)
```

Let's visualize what's happening to our spatial data:

```{r}
# Create a 2x2 plot showing population at different resolutions
par(mfrow = c(2, 2))
for(i in seq_along(results)) {
  plot(results[[i]]$pop, 
       main = sprintf("%0.0fm Resolution\n%s cells", 
                     results[[i]]$resolution,
                     format(results[[i]]$ncells, big.mark=",")))
  plot(vect(bound0), add = TRUE)
  plot(vect(hc12_hos[1:3,]), add = TRUE, pch = 16, col = "red")
}
```

What we're seeing here illustrates the fundamental trade-off in raster-based analysis:

#### Computational Impact

- Each doubling of resolution (halving cell size) quadruples the number of cells
- Processing time generally scales with cell count
- Memory and storage requirements grow quadratically with resolution

#### Precision Impact

- Coarser resolutions smooth out local population variations
- Facility locations snap to grid cells, affecting exact distances
- Catchment boundaries become more approximate
- But total population estimates remain surprisingly stable! (hopefully)

So Finding the Sweet Spot!

## The Best Practices: Optimizing Your Raster Workflow

Now that we understand both the power and limitations of raster-based analysis, let's look at concrete strategies for getting the most out of spax while avoiding common pitfalls.


### See Also:

-   [Getting Started with spax](https://songyosr.github.io/spax/articles/spax-101-intro.html)
-   [Data Preparation Guide](https://songyosr.github.io/spax/articles/spax-102-data-prep.html)
-   Dissecting spax: Understanding the package's inner workings *[under development]*
